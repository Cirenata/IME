{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cirenata/IME/blob/main/CSPortugu%C3%AAs_sem_preproc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q282wFOXceGu",
        "outputId": "d91f8a54-6428-43f3-fa09-7177a70ef5f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install ipython-autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h_azPEXdNlC"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statistics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpBRYted8g0x",
        "outputId": "587a9e39-9f46-40bb-cc14-4670bcdd3fd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 119 µs (started: 2022-04-01 14:29:14 +00:00)\n"
          ]
        }
      ],
      "source": [
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh0KpaAJR5HJ",
        "outputId": "ae59d68e-1a58-4a23-cd3f-9f179f73c28b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 872 ms (started: 2022-04-01 14:55:02 +00:00)\n"
          ]
        }
      ],
      "source": [
        "##### classes e funções auxiliares\n",
        "\n",
        "# classe do classificador \n",
        "class TextClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes = 2):\n",
        "    super(TextClassifier, self).__init__()\n",
        "\n",
        "    self.model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "    self.hidden = nn.Linear(768, 100)    \n",
        "    self.classes = nn.Linear(100, num_classes)\n",
        "\n",
        "    \n",
        "\n",
        "  def forward(self, input_id, mask):\n",
        "    _, pooled_output = self.model(input_ids = input_id, attention_mask = mask, return_dict = False)    \n",
        "    x = F.relu(self.hidden(pooled_output))\n",
        "    x = F.log_softmax(self.classes(x), dim = 1)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "# classe do Dataset, para carregar as informações\n",
        "# necessário dar margem senão dá erro no pytorch. Colocar padding para que todos os vetores sejam do mesmo tamanho\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, df, collabels, coltext):\n",
        "    # mapeando os valores da coluna de rótulos no dataframe para valores numéricos\n",
        "    uniques = dict()\n",
        "    anc = df[collabels].unique()\n",
        "    anc.sort()\n",
        "    for i, val in enumerate(anc):\n",
        "      uniques[val] = i\n",
        "    self.uniques = uniques\n",
        "    self.labels = [uniques[label] for label in df[collabels]]\n",
        "    self.texts = [tokenizer(text, padding = 'max_length', max_length = 512,\n",
        "                            truncation = True, return_tensors = 'pt') for text in df[coltext]]\n",
        "\n",
        "  def classes(self):\n",
        "    return self.labels\n",
        "\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  \n",
        "  def get_batch_labels(self, idx):\n",
        "    #fetch a batch of labels\n",
        "    return np.array(self.labels[idx])\n",
        "\n",
        "\n",
        "  def get_batch_texts(self, idx):\n",
        "    #fetch a batch of inputs\n",
        "    return self.texts[idx]\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):    \n",
        "    batch_texts = self.get_batch_texts(idx)\n",
        "    batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "    return batch_texts, batch_y\n",
        "\n",
        "# loop de treino\n",
        "def train_test(model, df, collabels, coltext, learning_rate = 0.001, epochs = 2, split = [80, 20], balance = True):\n",
        "  print(f'\\nTraining\\\n",
        "          \\nTraining Label Column: {collabels}\\\n",
        "          \\nLearning Rate: {learning_rate}\\\n",
        "          \\nEpochs: {epochs}\\n')\n",
        "\n",
        "  # estabelecendo o número de classes\n",
        "  num_classes = len(df[collabels].unique())  \n",
        "  \n",
        "  # Funções de perda e otimização\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = Adam(model.parameters(), lr = learning_rate, eps = 1e-7)\n",
        "  \n",
        "  # Avaliar o uso de GPU ou CPU, a ser usado com 'to(device)' para a conversão\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device('cuda' if use_cuda else 'cpu')  \n",
        "  if use_cuda:\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "  \n",
        "  # definição do tipo de validação:  \n",
        "  if type(split) is list and sum(split) == 100:\n",
        "    if len(split) == 2: \n",
        "      val_type = 0          # val_type = 0 - lista de tamanho 2 - Apenas treino e teste\n",
        "      print(f'Train/ Test training.')\n",
        "    elif len(split) == 3:\n",
        "      val_type = 1          # val_type = 1 - lista de tamanho 3 - Treino/ Validação / Teste\n",
        "      print(f'Train/ Validation/ Test training.')\n",
        "  elif type(split) is int and split < df.shape[0]/4:\n",
        "    val_type = 2            # val_type = 2 - inteiro - K-fold Cross-Validation\n",
        "    print('K-fold Cross-Validation training.')\n",
        "  else:\n",
        "    raise ValueError('The split value should be an integer (for K-Fold Cross-Validation),'\\\n",
        "                     'The integer must be less than 1/4 the dataset size (fold size at least 4)'\\\n",
        "                     'a list of length 2 (for train/test) or a list of length 3 (for train/validation/test)'\\\n",
        "                     'Split values list should reflect percentage, summing up 100')\n",
        "\n",
        "  # Criando um df para cada valor de rótulo. Serão proporcionalmente concatenados conforme a distribuição do split,\n",
        "  # a fim de manter a proporção. O dataloader se encarregará de embaralhá-los mais à frente.\n",
        "  subdf = dict()\n",
        "  sizes = dict()\n",
        "  for key in df[collabels].unique():\n",
        "    subdf[key] = df[df[collabels] == key]\n",
        "    sizes[key] = len(subdf[key])\n",
        "  # balanceamento pelo método da redução ao tamanho da menor classe\n",
        "  if balance:\n",
        "    for key in subdf.keys():\n",
        "      subdf[key] = subdf[key].iloc[:min(sizes.values())]\n",
        "      sizes[key] = len(subdf[key])\n",
        "  del df\n",
        "  \n",
        "  # preparando o split do dataset \n",
        "  # lista placeholder para as divisões de treino (, validação) e teste\n",
        "  folds = []       \n",
        "   \n",
        "  marks = [0]    \n",
        "  for mark in (range(split) if val_type == 2 else split):\n",
        "    # lista dos marcos de início e fim das divisões em split, para int e para list\n",
        "    marks.append(marks[-1] + (round(100/split, 5) if val_type == 2 else mark ))\n",
        "  for i in (range(split) if val_type == 2 else range(len(split))):\n",
        "    # lista em que vamos juntar os df dos folds de cada valor de rótulo\n",
        "    join = []\n",
        "    # para cada df rótulo\n",
        "    for key in subdf.keys():\n",
        "      # anexar um daquele rótulo\n",
        "      join.append(\n",
        "          subdf[key].iloc[\n",
        "                          # do começo daquela divisão, proporcional ao tamanho do df de rótulo\n",
        "                          math.floor(marks[i]/100*sizes[key])\n",
        "                          : # até\n",
        "                          # o fim daquela divisão, proporcional ao tamanho do df de rótulo\n",
        "                          math.ceil(marks[i+1]/100*sizes[key])\n",
        "                          ]\n",
        "          )\n",
        "    # com um df de rótulos de cada tipo, com tamanho proporcional ao fold, podemos concatená-los em um df\n",
        "    join = pd.concat(join) \n",
        "    folds.append(join)\n",
        " \n",
        "  del subdf, sizes    \n",
        "\n",
        "  ### treino\n",
        "  print('Starting training')\n",
        "  for turn_num in range(split if val_type == 2 else 1):\n",
        "    print(f'Round: {turn_num + 1 if val_type == 2 else 1} of {split if val_type == 2 else 1}')\n",
        "    # O conjunto de teste é o último da lista folds\n",
        "    test_index = -1\n",
        "    \n",
        "    # se a validação for kfold, o conjunto de teste varia por turno\n",
        "    if val_type == 2:\n",
        "      test_index = turn_num\n",
        "    test_data = folds.pop(test_index)    \n",
        "    \n",
        "    # se for Train/ Validation/ Test validation, retirar os dados de validação\n",
        "    if val_type == 1:\n",
        "      val_data = folds.pop(-1)\n",
        "    \n",
        "    # carregando os dados de treino (o restante), datasets e dataloaders\n",
        "    folds_datasets = []\n",
        "    for fold in folds:  \n",
        "      folds_datasets.append( \n",
        "          Dataset(fold, collabels, coltext)    \n",
        "      )\n",
        "    train = torch.utils.data.ConcatDataset(folds_datasets)    \n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size = 2, shuffle = True)\n",
        "    \n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "      total_acc_train = 0\n",
        "      total_loss_train = 0\n",
        "\n",
        "      for train_input, train_label in tqdm(train_dataloader):\n",
        "        train_label = train_label.to(device)\n",
        "        mask = train_input['attention_mask'].to(device)\n",
        "        input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "        output = model(input_id, mask)\n",
        "\n",
        "        batch_loss = criterion(output, train_label)\n",
        "        total_loss_train += batch_loss.item()\n",
        "\n",
        "        acc = (output.argmax(dim = 1) == train_label).sum().item()\n",
        "        total_acc_train += acc\n",
        "\n",
        "        model.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "      # validação da época no modo Train/Validation/Test\n",
        "      sitrep = f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train): .3f} \\\n",
        "            | Train Accuracy: {total_acc_train / len(train): .3f}' \n",
        "      if val_type == 1:\n",
        "        val_metrics = evaluate(model, val_data, collabels, coltext, criterion = criterion)\n",
        "        sitrep += f'| Val Loss: {val_metrics[\"Loss\"] / len(val_data): .3f if val_type == 1 else \"\"} \\\n",
        "            | Val Accuracy: {val_metrics[\"Accuracy\"]: .3f if val_type == 1 else \"\"}'\n",
        "      print(sitrep)        \n",
        "    # testando com o set de teste\n",
        "    metrics = evaluate(model, test_data, collabels, coltext)\n",
        "    if val_type == 2:      \n",
        "      # inicializando o dicionário de métricas acumuladas\n",
        "      if turn_num == 0:\n",
        "        acc_metrics = dict()\n",
        "        for key in metrics.keys():\n",
        "          acc_metrics[key] = []\n",
        "      for key in metrics.keys():\n",
        "        acc_metrics[key].append(metrics[key])\n",
        "      if metrics['Accuracy'] == max(acc_metrics['Accuracy']):\n",
        "        save_model(model, name= f'{split}fold-CS-{collabels.replace(\"/\",\"-\")}-LR{learning_rate}-Epochs{epochs}')\n",
        "      #model.zero_grad()\n",
        "      #optimizer.zero_grad()      \n",
        "      model = TextClassifier(num_classes = num_classes)\n",
        "      optimizer = Adam(model.parameters(), lr = learning_rate, eps = 1e-7)\n",
        "      if use_cuda:\n",
        "        model = model.cuda()\n",
        "      #repondo o fold de treino para reiniciar o k-fold\n",
        "      folds.insert(test_index, test_data)\n",
        "  if val_type == 2:\n",
        "    for key in acc_metrics.keys():\n",
        "      print(f'Average {key}: {statistics.mean(acc_metrics[key])}')\n",
        "    print(f'Best Accuracy (saved model): { max(acc_metrics[\"Accuracy\"])}')\n",
        "\n",
        "#função de avaliação\n",
        "def evaluate(model, data, collabels, coltext, criterion = None):\n",
        "  print(f'Testing data for {collabels} labels')\n",
        "  test = Dataset(data, collabels, coltext)\n",
        "  test_dataloader = torch.utils.data.DataLoader(test, batch_size = 1)\n",
        "\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "  if use_cuda:\n",
        "    model = model.cuda()  \n",
        "  actuls = dict(data[collabels].value_counts())\n",
        "  trus = dict()         # Armazena predições corretas\n",
        "  fals = dict()         # Armazena predições incorretas\n",
        "  inds = dict()      # Armazena a classe correspondente a cada índice (em output.argmax)\n",
        "  uniques = test.uniques\n",
        "  for key in uniques.keys():\n",
        "    trus[key] = 0\n",
        "    fals[key] = 0\n",
        "    inds[uniques[key]] = key\n",
        "  eval_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for test_input, test_label in tqdm(test_dataloader):\n",
        "      test_label = test_label.to(device)\n",
        "      mask = test_input['attention_mask'].to(device)\n",
        "      input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "      # aqui model retorna a saída da rede neural, com 1 dimensão para cada classe.\n",
        "      output = model(input_id, mask)\n",
        "\n",
        "      if criterion:\n",
        "        batch_loss = criterion (output, test_label)\n",
        "        eval_loss += batch_loss.item()\n",
        "\n",
        "      pred = output.argmax(dim = 1).item()\n",
        "      # Se a predição for correta, soma à classe em trus. Senão, em fals\n",
        "      if pred == test_label:\n",
        "        trus[inds[pred]] += 1\n",
        "      else:\n",
        "        fals[inds[pred]] += 1\n",
        "  # dicionário de métricas\n",
        "  print('\\n')\n",
        "  metrics = {\n",
        "        'Accuracy' : sum(trus.values())/(len(test)),          \n",
        "    }\n",
        "  for key in trus.keys():\n",
        "    try:\n",
        "      metrics[f'Precision for \"{key}\"'] = trus[key]/(trus[key] + fals[key])\n",
        "    except ZeroDivisionError:\n",
        "      print(f\"Couldn't calculate Precision for '{key}'\")\n",
        "    try:\n",
        "      metrics[f'Recall for \"{key}\"'] = trus[key]/actuls[key]\n",
        "    except ZeroDivisionError:\n",
        "      print(f\"Couldn't calculate Recall for '{key}'\")\n",
        "    try:\n",
        "      metrics[f'F1 for \"{key}\"'] = 2 * (metrics[f'Precision for \"{key}\"'] * metrics[f'Recall for \"{key}\"']) / (metrics[f'Precision for \"{key}\"'] + metrics[f'Recall for \"{key}\"'])\n",
        "    except (ZeroDivisionError, KeyError):\n",
        "      print(f\"Couldn't calculate F1 for '{key}'\")  \n",
        "  if criterion:\n",
        "    metrics['Loss'] = eval_loss    \n",
        "  else:\n",
        "    for metric in metrics.keys():\n",
        "      print(f'{metric:=<30}: {metrics[metric]: .3f}')\n",
        "  return metrics\n",
        "    \n",
        "\n",
        "                      \n",
        "def save_model(model, name = 'TextClassifier'):\n",
        "  path = f'./drive/MyDrive/Pesquisa/{name}.pt'\n",
        "  torch.save(model.state_dict(), path )\n",
        "  print(f'Saved file as {path}') \n",
        "\n",
        "def load_model(name = 'TextCLassifier'):\n",
        "  path = f'./drive/MyDrive/Pesquisa/{name}.pt'\n",
        "  model = TextClassifier()\n",
        "  model.load_state_dict(torch.load(path))\n",
        "  model.eval()\n",
        "  print(f'Loaded file {name} from folder {path}')\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjahjjpodX7B",
        "outputId": "3ca21174-d353-45c0-b5d5-20a92fa3fcf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               FRASE  OBJ/SUBJ\n",
            "0  Os dois se mudam para uma casa com fama de má ...  objetiva\n",
            "1  Conta a historia de cada uma de seus momentos ...  objetiva\n",
            "2  Vamos sonhar juntos traz uma análise do que es...  objetiva\n",
            "3  Ainda arrasado após a perda da esposa, Tom Ken...  objetiva\n",
            "4  Vinte anos antes, um pervertido assassino em s...  objetiva\n",
            "time: 1.67 s (started: 2022-04-01 14:29:15 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# carregar os tokenizadores e os modeladores para empregar nas classes de RNA\n",
        "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "# rótulos das colunas utilizadas dos dataframes\n",
        "labels_col = 'OBJ/SUBJ'\n",
        "text_col = 'FRASE'\n",
        "# colocando o caminho em uma variável e carregando o dataset a partir do CSV e carregando o dataset\n",
        "end = './drive/MyDrive/Pesquisa/corpora/corpus_book_reviews_portuguese.csv'\n",
        "df = pd.read_csv(end, usecols = [text_col , labels_col]) \n",
        "print(df.head(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ouyJJJ0nDQa"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlWQNDFF6zVa",
        "outputId": "b66601ca-ab95-4be0-e113-491e604141b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.65 s (started: 2022-04-01 14:29:17 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# modelo. irá baixar o automodel para o BERT\n",
        "model = TextClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-C_ILB5dmWw",
        "outputId": "1986c1e3-f4d2-4b42-e1a6-10301a9b6417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training          \n",
            "Training Label Column: OBJ/SUBJ          \n",
            "Learning Rate: 0.001          \n",
            "Epochs: 20\n",
            "\n",
            "K-fold Cross-Validation training.\n",
            "Starting training\n",
            "Round: 1 of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.347             | Train Accuracy:  0.466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:48<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss:  0.347             | Train Accuracy:  0.449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:50<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3 | Train Loss:  0.347             | Train Accuracy:  0.458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:50<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4 | Train Loss:  0.347             | Train Accuracy:  0.458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5 | Train Loss:  0.347             | Train Accuracy:  0.449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 6 | Train Loss:  0.347             | Train Accuracy:  0.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 7 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 8 | Train Loss:  0.347             | Train Accuracy:  0.483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 9 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 10 | Train Loss:  0.347             | Train Accuracy:  0.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 11 | Train Loss:  0.347             | Train Accuracy:  0.483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 12 | Train Loss:  0.347             | Train Accuracy:  0.492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:48<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 13 | Train Loss:  0.347             | Train Accuracy:  0.483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 14 | Train Loss:  0.347             | Train Accuracy:  0.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 15 | Train Loss:  0.347             | Train Accuracy:  0.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 16 | Train Loss:  0.347             | Train Accuracy:  0.492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:48<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 17 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 18 | Train Loss:  0.347             | Train Accuracy:  0.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:48<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 19 | Train Loss:  0.347             | Train Accuracy:  0.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 20 | Train Loss:  0.347             | Train Accuracy:  0.458\n",
            "Testing data for OBJ/SUBJ labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:08<00:00, 14.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Couldn't calculate Precision for 'objetiva'\n",
            "Couldn't calculate F1 for 'objetiva'\n",
            "Accuracy======================:  0.500\n",
            "Recall for \"objetiva\"=========:  0.000\n",
            "Precision for \"subjetiva\"=====:  0.500\n",
            "Recall for \"subjetiva\"========:  1.000\n",
            "F1 for \"subjetiva\"============:  0.667\n",
            "Saved file as ./drive/MyDrive/Pesquisa/3fold-CS-OBJ-SUBJ-LR0.001-Epochs20.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round: 2 of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.379             | Train Accuracy:  0.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss:  0.355             | Train Accuracy:  0.551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3 | Train Loss:  0.351             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:48<00:00,  2.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:48<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 6 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 7 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 8 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 9 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:50<00:00,  2.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 10 | Train Loss:  0.347             | Train Accuracy:  0.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:50<00:00,  2.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 11 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 12 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 13 | Train Loss:  0.347             | Train Accuracy:  0.458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 14 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 15 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 16 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 17 | Train Loss:  0.347             | Train Accuracy:  0.449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 18 | Train Loss:  0.347             | Train Accuracy:  0.483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 19 | Train Loss:  0.347             | Train Accuracy:  0.458\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 20 | Train Loss:  0.347             | Train Accuracy:  0.449\n",
            "Testing data for OBJ/SUBJ labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:08<00:00, 14.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Couldn't calculate Precision for 'objetiva'\n",
            "Couldn't calculate F1 for 'objetiva'\n",
            "Accuracy======================:  0.500\n",
            "Recall for \"objetiva\"=========:  0.000\n",
            "Precision for \"subjetiva\"=====:  0.500\n",
            "Recall for \"subjetiva\"========:  1.000\n",
            "F1 for \"subjetiva\"============:  0.667\n",
            "Saved file as ./drive/MyDrive/Pesquisa/3fold-CS-OBJ-SUBJ-LR0.001-Epochs20.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round: 3 of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.369             | Train Accuracy:  0.492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 2 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 4 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 5 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 6 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 7 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 8 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:50<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 9 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 10 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 11 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 12 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 13 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 14 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:50<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 15 | Train Loss:  0.347             | Train Accuracy:  0.475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:50<00:00,  2.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 16 | Train Loss:  0.347             | Train Accuracy:  0.500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 17 | Train Loss:  0.347             | Train Accuracy:  0.432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 18 | Train Loss:  0.347             | Train Accuracy:  0.441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 19 | Train Loss:  0.347             | Train Accuracy:  0.424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:49<00:00,  2.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 20 | Train Loss:  0.347             | Train Accuracy:  0.500\n",
            "Testing data for OBJ/SUBJ labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:08<00:00, 14.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Couldn't calculate Precision for 'objetiva'\n",
            "Couldn't calculate F1 for 'objetiva'\n",
            "Accuracy======================:  0.500\n",
            "Recall for \"objetiva\"=========:  0.000\n",
            "Precision for \"subjetiva\"=====:  0.500\n",
            "Recall for \"subjetiva\"========:  1.000\n",
            "F1 for \"subjetiva\"============:  0.667\n",
            "Saved file as ./drive/MyDrive/Pesquisa/3fold-CS-OBJ-SUBJ-LR0.001-Epochs20.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.5\n",
            "Average Recall for \"objetiva\": 0.0\n",
            "Average Precision for \"subjetiva\": 0.5\n",
            "Average Recall for \"subjetiva\": 1.0\n",
            "Average F1 for \"subjetiva\": 0.6666666666666666\n",
            "Best Accuracy (saved model): 0.5\n",
            "time: 50min 9s (started: 2022-04-01 14:55:18 +00:00)\n"
          ]
        }
      ],
      "source": [
        "%timeit\n",
        "# Treinando a rede neural\n",
        "\n",
        "# parâmetros do treino\n",
        "EPOCHS = 20\n",
        "LR = 1e-3\n",
        "\n",
        "# treinamento do modelo\n",
        "train_test(model, df, labels_col, text_col, learning_rate = LR, epochs = EPOCHS, split = 3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0BOkAnsqQrV"
      },
      "outputs": [],
      "source": [
        "'''# Teste para 3 classes com 80/20\n",
        "df2 = df\n",
        "df2.iloc[118:234] = df2.iloc[118:234].replace(to_replace = ['objetiva', 'subjetiva'], value = ['indecisa']*2)\n",
        "df2'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e0OvmyKueRG"
      },
      "outputs": [],
      "source": [
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfWufaoy1GE2"
      },
      "outputs": [],
      "source": [
        "'''# modelo para treino com 3 classes\n",
        "model2 = TextClassifier(num_classes = 3)\n",
        "train_test(model2, df2, labels_col, text_col, learning_rate = 0.0001, epochs = 10 )'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpqhFv1JViFZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH4rJojxV_ox"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CSPortuguês - sem preproc.ipynb",
      "provenance": [],
      "mount_file_id": "15ioJWJCXVr3xDpDJhbZsRQjxH_GAZUiJ",
      "authorship_tag": "ABX9TyN2f3xtF9zRAxq+OpJ+P5io",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}